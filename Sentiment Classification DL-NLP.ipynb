{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout,GRU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Here positive, negative and unsupervised reviews are extracted, labeled and stored in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = 'data/'\n",
    "pfile = 'positive_reviews.txt'\n",
    "nfile = 'negative_reviews.txt'\n",
    "unfile = 'unsupervised_reviews.txt'\n",
    "\n",
    "with open(origin+pfile, encoding=\"latin1\") as f:\n",
    "        positiveReviews = f.read().splitlines()\n",
    "with open(origin+nfile, encoding=\"latin1\") as f:\n",
    "        negativeReviews = f.read().splitlines()\n",
    "with open(origin+unfile, encoding=\"latin1\") as f:\n",
    "        unsupervisedReviews = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33226</th>\n",
       "      <td>portly nice guy falls for a luscious blonde sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64804</th>\n",
       "      <td>new york minute is a summer movie for to year...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39763</th>\n",
       "      <td>some movies you watch and you say well that ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51270</th>\n",
       "      <td>a pretty good film i really loved the cast rat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9698</th>\n",
       "      <td>although i was born in the year that this movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "33226  portly nice guy falls for a luscious blonde sh...      0\n",
       "64804   new york minute is a summer movie for to year...     -1\n",
       "39763  some movies you watch and you say well that ma...      0\n",
       "51270  a pretty good film i really loved the cast rat...     -1\n",
       "9698   although i was born in the year that this movi...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.concat([\n",
    "    pd.DataFrame({\"review\":positiveReviews, \"label\":1}),\n",
    "    pd.DataFrame({\"review\":negativeReviews, \"label\":0}),\n",
    "    pd.DataFrame({\"review\":unsupervisedReviews, \"label\":-1})\n",
    "], ignore_index=True).sample(frac=1, random_state=10)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train, validation and test sets\n",
    "\n",
    "Here sentences are split into training, validation and testing datasets. Additionally, the proportions of positive/negative reviews in the dataset are checked for balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[[\"review\", \"label\"]].sample(frac=1, random_state=1)\n",
    "\n",
    "#training set\n",
    "train = reviews[reviews.label!=-1].sample(frac=0.6, random_state=1)\n",
    "\n",
    "non_train = reviews[reviews.label!=-1].drop(train.index)\n",
    "\n",
    "#validation set\n",
    "valid = non_train.sample(frac=0.5, random_state=1)\n",
    "\n",
    "#test set\n",
    "test = non_train.drop(valid.index)\n",
    "\n",
    "unsu = reviews[reviews.label==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n",
      "(10000, 2)\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "#  Checking shapes\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5016 0.5009 0.4943\n"
     ]
    }
   ],
   "source": [
    "# Checking class balance, seems balanced enough\n",
    "print(train[\"label\"].mean(), valid[\"label\"].mean(), test[\"label\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34644</th>\n",
       "      <td>it s interesting to see what shape pierce bros...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28999</th>\n",
       "      <td>this film was choppy incoherent and contrived ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18124</th>\n",
       "      <td>let s start from this point this is not a movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37706</th>\n",
       "      <td>im warning you people out there this is just a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45566</th>\n",
       "      <td>this film is basically a poor take on the old ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "34644  it s interesting to see what shape pierce bros...      0\n",
       "28999  this film was choppy incoherent and contrived ...      0\n",
       "18124  let s start from this point this is not a movi...      1\n",
       "37706  im warning you people out there this is just a...      0\n",
       "45566  this film is basically a poor take on the old ...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is the worst movie i have ever seen the story line is a joke the effects are terrible the cinematography doesn t fit the tone of the movie the dialogue is cheesy and the actors do a good job at screwing up the rest people just don t act that way in real life situations my question is who would fund such crap the movie starts where some miners fall down a mine shaft after a fireman fails to save them next we join some bikers in a forest who ride around doing stunts on their bikes one guy falls and breaks his leg or something the fireman arrives to help them meanwhile somebody starts a fire some more bike stunts bla bla bla i wasted my time do not watch this movie '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.review.iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalisation (Preprocessing)\n",
    "\n",
    "I carried out the following tech normalisation techniques:\n",
    "\n",
    "\n",
    "* Tokenized the reviews\n",
    "* Removed stopwords, due to their lack of semantic content\n",
    "* Lemmatisation using wordnet to undo word inflections and map words back to their roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34644    it s interesting to see what shape pierce bros...\n",
       "28999    this film was choppy incoherent and contrived ...\n",
       "18124    let s start from this point this is not a movi...\n",
       "37706    im warning you people out there this is just a...\n",
       "45566    this film is basically a poor take on the old ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Combining DFs for cleaning\n",
    "df = train.append(valid).append(test)\n",
    "df['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lines = []\n",
    "review_lines_strings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for line in df['review'].values.tolist():\n",
    "    words = word_tokenize(line)\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if not w in stop_words] # lemmatizing and removing stopwords\n",
    "    review_lines.append(words)\n",
    "    review_lines_strings.append(\" \".join(words))\n",
    "\n",
    "df['review'] = review_lines_strings # Updating df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_lines_strings[650])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  splitting up the dataframe again after cleaning\n",
    "x_train = df[\"review\"][:train.shape[0]]\n",
    "y_train = df[\"label\"][:train.shape[0]]\n",
    "\n",
    "x_valid = df[\"review\"][train.shape[0]:train.shape[0] + valid.shape[0]]\n",
    "y_valid = df[\"label\"][train.shape[0]:train.shape[0] + valid.shape[0]]\n",
    "\n",
    "x_test = df[\"review\"][train.shape[0] + valid.shape[0]:]\n",
    "y_test = df[\"label\"][train.shape[0] + valid.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Multinomial Naive Bayes Baseline\n",
    "\n",
    "This is based on a simple generative classifier that is using counts for each word in each review (Bag of words representations) and modelling this as multinomially ditributed iid data and then fitting these conditional distributions on each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_all = vectorizer.fit_transform( df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 89978) (10000, 89978) (10000, 89978)\n"
     ]
    }
   ],
   "source": [
    "#  splitting up the count vectorizer data a\n",
    "x_train_c = X_all[:train.shape[0], :]\n",
    "\n",
    "x_valid_c = X_all[train.shape[0]:train.shape[0] + valid.shape[0], :]\n",
    "\n",
    "x_test_c = X_all[train.shape[0] + valid.shape[0]:, :]\n",
    "\n",
    "print(x_train_c.shape, x_valid_c.shape, x_test_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for multinomial NB 0.8571\n"
     ]
    }
   ],
   "source": [
    "model_nb = MultinomialNB()\n",
    "\n",
    "model_nb.fit(x_train_c, y_train)\n",
    "\n",
    "test_accuracy = model_nb.score(x_test_c, y_test)\n",
    "\n",
    "print(f'Test accuracy for multinomial NB {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embedding and LSTM\n",
    "\n",
    "Here I employ an LSTM taking in a sequence (time-series) of word embeddings as input, since the  LSTM is a model with very low bias it is prone to overfitting thus  regularisation techniques such as dropout, L1, L2, and early stopping are needed, fitting the hyperparameters of these techniques require a validation set. I did not pursue any methods such as grid searches, random search or bayesian optimisation to tune the regularisers due to lack of time, instead I just eyeballed it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = Word2Vec(review_lines, size=EMB_DIM, min_count = 1, workers=5, window = 5, sg=0, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: 90032\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of word vectors: {}\".format(len(word_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('decent', 0.8301441669464111), ('great', 0.8031619787216187), ('bad', 0.7798143625259399), ('ok', 0.7188183069229126), ('nice', 0.7141984701156616), ('fine', 0.7071343660354614), ('okay', 0.703923761844635), ('cool', 0.702067494392395), ('alright', 0.6900888085365295), ('awesome', 0.6599547863006592)]\n"
     ]
    }
   ],
   "source": [
    "print(word_model.wv.most_similar('good')) # Quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer( char_level=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "seq_valid = tokenizer.texts_to_sequences(x_valid)\n",
    "seq_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train = pad_sequences(seq_train, maxlen=SEQUENCE_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "review_valid = pad_sequences(seq_valid, maxlen=SEQUENCE_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "review_test = pad_sequences(seq_test, maxlen=SEQUENCE_LENGTH,\n",
    "                     padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tensor:      (30000, 500)\n",
      "Validation tensor: (10000, 500)\n",
      "Test tensor:       (10000, 500)\n"
     ]
    }
   ],
   "source": [
    "print('Train tensor:     ', review_train.shape)\n",
    "print('Validation tensor:', review_valid.shape)\n",
    "print('Test tensor:      ', review_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 72474\n"
     ]
    }
   ],
   "source": [
    "print('Number of words', len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not find 50 words out of 72475 thats 0.0689893066574681 percent gone\n"
     ]
    }
   ],
   "source": [
    "#  Initialising the embedding layer to use word2vec\n",
    "\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "# we initialize the matrix with zeros\n",
    "word_vect_matrix = np.zeros((num_words, EMB_DIM))\n",
    "num_zeros = 0 \n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        # words not found in embedding index will be initialised to zero (the model can learn to ignore these)\n",
    "        word_vect_matrix[i] = word_model.wv[word]\n",
    "    except KeyError:\n",
    "        # If a key exception happens word embedding will be a zero vector\n",
    "        num_zeros += 1\n",
    "        pass\n",
    "\n",
    "percent_loss = num_zeros * 100.0 / num_words \n",
    "print(f\"did not find {num_zeros} words out of {num_words} thats {percent_loss} percent gone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 52)           3768700   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               92672     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,861,501\n",
      "Trainable params: 92,801\n",
      "Non-trainable params: 3,768,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                     EMB_DIM,\n",
    "                     mask_zero=False,\n",
    "                     weights=[word_vect_matrix],\n",
    "                     input_length=SEQUENCE_LENGTH,\n",
    "                     trainable=False))\n",
    "model.add(LSTM(128, kernel_regularizer=regularizers.l2(0.001), dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "30000/30000 [==============================] - 181s 6ms/step - loss: 0.5806 - acc: 0.7503 - val_loss: 0.4546 - val_acc: 0.8316\n",
      "Epoch 2/5\n",
      "30000/30000 [==============================] - 170s 6ms/step - loss: 0.4726 - acc: 0.8135 - val_loss: 0.4032 - val_acc: 0.8559\n",
      "Epoch 3/5\n",
      "30000/30000 [==============================] - 178s 6ms/step - loss: 0.4270 - acc: 0.8358 - val_loss: 0.3740 - val_acc: 0.8650\n",
      "Epoch 4/5\n",
      "30000/30000 [==============================] - 184s 6ms/step - loss: 0.3997 - acc: 0.8495 - val_loss: 0.3561 - val_acc: 0.8688\n",
      "Epoch 5/5\n",
      "30000/30000 [==============================] - 188s 6ms/step - loss: 0.3767 - acc: 0.8567 - val_loss: 0.3406 - val_acc: 0.8739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea5cdf34a8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 500 # tweaked a bit but mostly picked due to speed limitations\n",
    "model.fit(review_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(review_valid, y_valid))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 16s 2ms/step\n",
      "Test accuracy: 0.8786999970674515\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(review_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print(f'Test accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
